{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7182674c",
   "metadata": {},
   "source": [
    "## Parameter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68419902",
   "metadata": {},
   "source": [
    "#### Options for Conv1D\n",
    "\n",
    "Arguments\n",
    "\n",
    "* filters: Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).\n",
    "\n",
    "* kernel_size: An integer or tuple/list of a single integer, specifying the length of the 1D convolution window.\n",
    "\n",
    "* strides: An integer or tuple/list of a single integer, specifying the stride length of the convolution. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1.\n",
    "\n",
    "* padding: One of \"valid\", \"same\" or \"causal\" (case-insensitive). \"valid\" means no padding. \"same\" results in padding with zeros evenly to the left/right or up/down of the input such that output has the same height/width dimension as the input. \"causal\" results in causal (dilated) convolutions, e.g. output[t] does not depend on input[t+1:]. Useful when modeling temporal data where the model should not violate the temporal order. \n",
    "\n",
    "* data_format: A string, one of channels_last (default) or channels_first.\n",
    "\n",
    "* dilation_rate: an integer or tuple/list of a single integer, specifying the dilation rate to use for dilated convolution. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any strides value != 1.\n",
    "\n",
    "* activation: Activation function to use. If you don't specify anything, no activation is applied (see keras.activations).\n",
    "\n",
    "* use_bias: Boolean, whether the layer uses a bias vector.\n",
    "\n",
    "* kernel_initializer: Initializer for the kernel weights matrix (see keras.initializers). Defaults to 'glorot_uniform'.\n",
    "\n",
    "* bias_initializer: Initializer for the bias vector (see keras.initializers). Defaults to 'zeros'.\n",
    "\n",
    "* kernel_regularizer: Regularizer function applied to the kernel weights matrix (see keras.regularizers).\n",
    "\n",
    "* bias_regularizer: Regularizer function applied to the bias vector (see keras.regularizers).\n",
    "\n",
    "* activity_regularizer: Regularizer function applied to the output of the layer (its \"activation\") (see keras.regularizers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37540323",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = Conv1D(filters=4, kernel_size=4, strides=1, activation=\"relu\", name=\"Conv1D-1\")(inputSeq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1209236d",
   "metadata": {},
   "source": [
    "#### Options for max pool\n",
    "\n",
    "* pool_size: Integer, size of the max pooling window.\n",
    "\n",
    "* strides: Integer, or None. Specifies how much the pooling window moves for each pooling step. If None, it will default to pool_size.\n",
    "\n",
    "* padding: One of \"valid\" or \"same\" (case-insensitive). \"valid\" means no padding. \"same\" results in padding evenly to the left/right or up/down of the input such that output has the same height/width dimension as the input.\n",
    "\n",
    "* data_format: A string, one of channels_last (default) or channels_first. The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, steps, features) while channels_first corresponds to inputs with shape (batch, features, steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c72f98d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = MaxPooling1D(pool_size=4, name=\"MaxPool-1\")(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3733b1",
   "metadata": {},
   "source": [
    "#### Options for dense layer\n",
    "\n",
    "* units: Positive integer, dimensionality of the output space.\n",
    "\n",
    "* activation: Activation function to use. If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x).\n",
    "\n",
    "* use_bias: Boolean, whether the layer uses a bias vector.\n",
    "\n",
    "* kernel_initializer: Initializer for the kernel weights matrix.\n",
    "\n",
    "* bias_initializer: Initializer for the bias vector.\n",
    "\n",
    "* kernel_regularizer: Regularizer function applied to the kernel weights matrix.\n",
    "\n",
    "* bias_regularizer: Regularizer function applied to the bias vector.\n",
    "\n",
    "* activity_regularizer: Regularizer function applied to the output of the layer (its \"activation\").\n",
    "\n",
    "* kernel_constraint: Constraint function applied to the kernel weights matrix.\n",
    "\n",
    "* bias_constraint: Constraint function applied to the bias vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a226f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = Dense(8, activation=\"relu\", name=\"Dense-1\")(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70716864",
   "metadata": {},
   "source": [
    "#### Options for dropout\n",
    "\n",
    "* rate: Float between 0 and 1. Fraction of the input units to drop.\n",
    "\n",
    "* noise_shape: 1D integer tensor representing the shape of the binary dropout mask that will be multiplied with the input. For instance, if your inputs have shape (batch_size, timesteps, features) and you want the dropout mask to be the same for all timesteps, you can use noise_shape=(batch_size, 1, features).\n",
    "\n",
    "* seed: A Python integer to use as random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9cbabad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = Dropout(0.2, name=\"Dropout-1\")(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7702f95",
   "metadata": {},
   "source": [
    "#### Options for optimizers\n",
    "\n",
    "* SGD\n",
    "* RMSprop\n",
    "* Adam\n",
    "* Adadelta\n",
    "* Adagrad\n",
    "* Adamax\n",
    "* Nadam\n",
    "* Ftrl\n",
    "\n",
    "#### Options for probabilistic loss\n",
    "\n",
    "* BinaryCrossentropy class\n",
    "* CategoricalCrossentropy class\n",
    "* SparseCategoricalCrossentropy class\n",
    "* Poisson class\n",
    "* binary_crossentropy function\n",
    "* categorical_crossentropy function\n",
    "* sparse_categorical_crossentropy function\n",
    "* poisson function\n",
    "* KLDivergence class\n",
    "* kl_divergence function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e88b0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(optimizer=Adam(.001), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
